# -*- coding: utf-8 -*-
"""mymodules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hgzs5ErF3zYql2HSLk96iRad3aLQaGCW

##This file includes the functions that are called from the notebooks of satellite images classification.
"""

#Import libraries
import tensorflow as tf
import cv2
from sklearn.model_selection import train_test_split
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow_hub as hub
from tensorflow.python import metrics
from tensorflow.keras import models, layers
from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger

OUTPUT_PATH = "./drive/MyDrive/satellite_images_classification/output/"
SAVED_MODEL_PATH = OUTPUT_PATH + 'saved_models/'

IMAGE_SIZE = 224
BATCH_SIZE = 32

def retrieve_images_and_labels(data_path, unique_labels):
  """
  Retrieve and return the image file names (full path) and labels as arrays
  """
  image_names = []
  labels = []

  #retrieve image file names and labels
  for folder in os.listdir(data_path):
    for file in os.listdir(f'{data_path}/{folder}'):
      file = f'{data_path}/{folder}/{file}'
      image_names.append(file)
      labels.append(folder)
  image_names = np.array(image_names)
  
  #convert labels from strings to integers
  unique_labels_array = np.array(unique_labels)
  int_labels = np.array(list(map(lambda x: unique_labels.index(x), labels)))
  
  return image_names, int_labels

def plot_image_counts(unique_labels, labels):
  """
  plot the distribution of different classes in a bar plot
  """
  counts = list(map(lambda x: (labels==unique_labels.index(x)).sum(), unique_labels))
  print(counts)
  plt.bar(unique_labels, counts)
  plt.title('Number of samples per class', fontsize=16)
  plt.xlabel('Categories', fontsize=14)
  plt.ylabel('Counts', fontsize=14)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)

def process_image(filename, img_size = IMAGE_SIZE):
  """
  read in the image, rescale, resize
  also check if the image is empty (all zero values)
  """
  empty_count = 0
  img = cv2.imread(filename)
  # rescale
  img = np.asarray(img)/255
  img = tf.convert_to_tensor(img)
  if (tf.reduce_sum(img) == 0) :
    empty_count += 1
    print('!!!Stop: empty images are found:', filename)
  # resize
  img = tf.image.resize(img, size=[img_size, img_size])
  return img

def get_dataset(image_arr, label_arr):
  """
  run function "process_image" on the input image array,
  convert the processed images and labels to tensorflow datasets,
  and combine the image tf dataset with the label tf dataset
  """
  images = map(process_image, image_arr)
  image_tensor = tf.data.Dataset.from_tensor_slices(list(images))
  label_tensor = tf.data.Dataset.from_tensor_slices(list(label_arr))
  ds_zip = tf.data.Dataset.zip((image_tensor, label_tensor))
  return ds_zip

def get_loss_accu(history):
  """
  retrieve loss function and accuracy from a model history
  """
  loss_train = history.history['loss']
  accu_train = history.history['accuracy']
  loss_valid = history.history['val_loss']
  accu_valid = history.history['val_accuracy']
  return loss_train, accu_train, loss_valid, accu_valid

def plot_loss_accu(epoch, loss_tr, accu_tr, loss_v, accu_v):
  fig, axes = plt.subplots(1,2,figsize=(10,3))
  axes[0].plot(range(0, epoch), loss_tr, label='train')
  axes[0].plot(range(0, epoch), loss_v, label='valid')
  axes[0].legend()
  axes[1].plot(range(0, epoch), accu_tr, label='train')
  axes[1].plot(range(0, epoch), accu_v, label='valid')
  axes[1].legend()
  axes[0].set_title('Loss function: training set and valid set')
  axes[1].set_title('Accuracy: training set and valid set')

def get_real_labels(ds):
  """
  extract real labels from a dataset
  """
  labels=[]
  for img, lbl in ds.as_numpy_iterator():
    labels=labels+list(lbl)
  return np.array(labels)

def check_model_accuracy(ds, pred):
  """
  retrieve the real labels, and calculate the prediction accuracy for each class 
  """
  y = get_real_labels(ds)

  def get_accuracy(real, pred):
    #calculate the prediction accuracy for each class
    correctPrediction = [r for r, p in zip(real, pred) if r==p]
    accu = [correctPrediction.count(i)/list(real).count(i) for i in range(len(set(real)))]
    return np.round(accu, 2)  

  pred_accu = get_accuracy(y, pred)
  return pred_accu

def get_predict_result(model, train_ds, valid_ds, test_ds):
  """
  run model prediction, and calculate the accuracy for each dataset and each class
  """
  pred_train = model.predict(train_ds)
  pred_valid = model.predict(valid_ds)
  pred_test = model.predict(test_ds)
  pred_train = np.array([np.argmax(p) for p in pred_train])
  pred_valid = np.array([np.argmax(p) for p in pred_valid])
  pred_test  = np.array([np.argmax(p) for p in pred_test])  

  pred_accu_train_base = check_model_accuracy(train_ds, pred_train)
  pred_accu_valid_base = check_model_accuracy(valid_ds, pred_valid)
  pred_accu_test_base = check_model_accuracy(test_ds, pred_test)
  print('training set accuracy for each class:  ', list(pred_accu_train_base))
  print('validation set accuracy for each class:', list(pred_accu_valid_base))
  print('test set accuracy for each class:      ', list(pred_accu_test_base))

def build_train_with_pretrained_model(train_ds, valid_ds, test_ds, model, 
                          modelname, input_shape, nclasses, callbacks,lr=1e-4, epochs=15):
  """ 
  #This function builds a model from a pretrained model, 
  #compiles and trains the model,
  #save the trained model
  #returns the predicted labels for training, validation, and test sets
  """

  # set the base model as the pretrained model, freeze the parameters in this layer
  pretrained_model = hub.KerasLayer(model, input_shape=input_shape)
  pretrained_model.trainable = False

  # build the model
  if nclasses==1:
    lastAct = 'sigmoid'
  else:
    lastAct = 'softmax'

  classifier = tf.keras.Sequential([
      pretrained_model,
      layers.Dense(64, activation="relu"), # Hidden Layer   
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(64, activation="relu"), # Hidden Layer       
      tf.keras.layers.Dropout(0.2),
      tf.keras.layers.Dense(nclasses, activation=lastAct)
  ]) # Layer 4 (Output Layer)       

  # compile
  classifier.compile(
      optimizer = tf.keras.optimizers.Adam(learning_rate=lr),
      loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),
      metrics = ['accuracy'])

  # train
  history_pretrained = classifier.fit(train_ds, 
                    epochs = epochs, 
                    batch_size=BATCH_SIZE, 
                    verbose=0,
                    validation_data=valid_ds,
                    callbacks=callbacks)

  loss_train, accu_train, loss_valid, accu_valid = get_loss_accu(history_pretrained)
  total_epoch = len(loss_train)
  plot_loss_accu(total_epoch, loss_train, accu_train, loss_valid, accu_valid)

  #evaluate
  test_loss, test_accu = classifier.evaluate(test_ds)
  print("Pretrain model {}: test dataset: loss={}, accuracy={}".format
          (model, test_loss, test_accu)) 

  #predict
  pred_train = classifier.predict(train_ds)
  pred_valid = classifier.predict(valid_ds)
  pred_test = classifier.predict(test_ds)

  pred_train = np.array([np.argmax(p) for p in pred_train])
  pred_valid = np.array([np.argmax(p) for p in pred_valid])
  pred_test  = np.array([np.argmax(p) for p in pred_test])

  #save model
  classifier.save(SAVED_MODEL_PATH + modelname)

  return pred_train, pred_valid, pred_test

