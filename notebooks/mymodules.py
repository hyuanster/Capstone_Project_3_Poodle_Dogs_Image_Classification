# -*- coding: utf-8 -*-
"""mymodules.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hgzs5ErF3zYql2HSLk96iRad3aLQaGCW

##This file includes the functions that are called from the notebooks of satellite images classification.
"""

#Import libraries
import tensorflow as tf
import cv2
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
from tensorflow.keras import models, layers
import seaborn as sns

IMAGE_SIZE = 224

def retrieve_images_and_labels(data_path, unique_labels):
  """
  Retrieve and return the image file names (full path) and labels as arrays
  """
  image_names = []
  labels = []

  #retrieve image file names and labels
  for folder in os.listdir(data_path):
    for file in os.listdir(f'{data_path}/{folder}'):
      file = f'{data_path}/{folder}/{file}'
      image_names.append(file)
      labels.append(folder)
  image_names = np.array(image_names)
  
  #convert labels from strings to integers
  unique_labels_array = np.array(unique_labels)
  int_labels = np.array(list(map(lambda x: unique_labels.index(x), labels)))
  
  return image_names, int_labels

def plot_image_counts(unique_labels, labels):
  """
  plot the distribution of different classes in a bar plot
  """
  plt.subplots(figsize=(10,4))  
  plt.bar(unique_labels, counts, width=0.5)
  counts = list(map(lambda x: (labels==unique_labels.index(x)).sum(), unique_labels))
  print(counts)
  plt.bar(unique_labels, counts)
  plt.title('Number of samples per class', fontsize=16)
  plt.xlabel('Categories', fontsize=14)
  plt.ylabel('Counts', fontsize=14)
  plt.xticks(fontsize=12)
  plt.yticks(fontsize=12)

def process_image(filename, img_size = IMAGE_SIZE):
  """
  read in the image, rescale, resize
  also check if the image is empty (all zero values)
  """
  empty_count = 0
  img = cv2.imread(filename)
  # rescale
  img = np.asarray(img)/255
  img = tf.convert_to_tensor(img)
  if (tf.reduce_sum(img) == 0) :
    empty_count += 1
    print('!!!Stop: empty images are found:', filename)
  # resize
  img = tf.image.resize(img, size=[img_size, img_size])
  return img

def get_dataset(image_arr, label_arr):
  """
  run function "process_image" on the input image array,
  convert the processed images and labels to tensorflow datasets,
  and combine the image tf dataset with the label tf dataset
  """
  images = map(process_image, image_arr)
  image_tensor = tf.data.Dataset.from_tensor_slices(list(images))
  label_tensor = tf.data.Dataset.from_tensor_slices(list(label_arr))
  ds_zip = tf.data.Dataset.zip((image_tensor, label_tensor))
  return ds_zip

"""### Plot loss function, accuracy"""

def plot_loss_accu(modelname, epoch, loss_tr, accu_tr, loss_v, accu_v, fname):
  fig, axes = plt.subplots(1,2,figsize=(10,4))
  axes[0].plot(range(0, epoch), np.round(loss_tr,2), label='train')
  axes[0].plot(range(0, epoch), np.round(loss_v,2), label='valid')
  axes[0].legend()
  axes[1].plot(range(0, epoch), np.round(accu_tr,2), label='train')
  axes[1].plot(range(0, epoch), np.round(accu_v,2), label='valid')
  axes[1].legend()
  axes[0].set_title(modelname+': loss function')
  axes[1].set_title(modelname+': accuracy') 
  axes[0].grid(axis='y')
  axes[1].grid(axis='y')
  axes[0].set_xlabel('epoch')
  axes[1].set_xlabel('epoch')
  axes[0].set_ylabel('loss function')
  axes[1].set_ylabel('accuracy')
  plt.savefig(fname)

def check_loss_accu(modelname, history, fname):
  loss_tr = history.history['loss']
  accu_tr = history.history['accuracy']
  loss_val = history.history['val_loss']
  accu_val = history.history['val_accuracy']  

  epoch = len(loss_tr)
  plot_loss_accu(modelname, epoch, loss_tr, accu_tr, loss_val, accu_val, fname)

"""### Check model prediction accuracy"""

def get_accuracy(y, pred):
  """
  retrieve the real labels, and calculate the prediction accuracy for each class 
  """
  correctPrediction = [r for r, p in zip(y, pred) if r==p]
  accu = [correctPrediction.count(i)/list(y).count(i) for i in range(len(set(y)))]

  return np.round(accu, 2)

def get_predict_result(model, x_tr, x_val, x_test, nclasses):
  """
  run model prediction, and calculate the accuracy for each dataset and each class
  """
  if nclasses==1 :
    print('yes')
    pred_train = np.int8(np.array([p>0.5 for p in  model.predict(x_tr)]))
    pred_valid = np.int8(np.array([p>0.5 for p in model.predict(x_val)]))
    pred_test  = np.int8(np.array([p>0.5 for p in model.predict(x_test)]))  
  else:
    pred_train = np.array([np.argmax(p) for p in model.predict(x_tr)])
    pred_valid = np.array([np.argmax(p) for p in model.predict(x_val)])
    pred_test  = np.array([np.argmax(p) for p in  model.predict(x_test)])  
  return pred_train, pred_valid, pred_test

def check_prediction_accuracy(modelname, y_tr, y_val, y_test, pred_tr, pred_val, pred_test, nclasses):
  
  # get prediction accuracy for each class
  accu_dic = dict.fromkeys(["train", "valid", "test"], None)
  accu_dic['train'] = get_accuracy(y_tr, pred_tr)
  accu_dic['valid'] = get_accuracy(y_val, pred_val)
  accu_dic['test']  = get_accuracy(y_test, pred_test)

  df = pd.DataFrame(accu_dic)
  if nclasses == 1:
    lbls = np.arange(0, nclasses+1)
  else :
    lbls = np.arange(0, nclasses)
  df['class'] = list(lbls)
  df.set_index('class', inplace=True)
  print('prediction accuracy of {}'.format(modelname))
  print(df)

"""### Model creation and training"""

def build_train_with_pretrained_model(train_ds, valid_ds, test_ds, model, 
                          modelname, input_shape, nclasses, callbacks,
                          lr=1e-4, epochs=15, batch_size=32):
  """ 
  #This function builds a model from a pretrained model, 
  #compiles and trains the model,
  #save the trained model
  #returns the predicted labels for training, validation, and test sets
  """

  # set the base model as the pretrained model, freeze the parameters in this layer
  pretrained_model = hub.KerasLayer(model, input_shape=input_shape)
  pretrained_model.trainable = False

  # build the model
  if nclasses==1:
    lastAct = 'sigmoid'
  else:
    lastAct = 'softmax'

  classifier = tf.keras.Sequential([
      pretrained_model,
      layers.Dense(64, activation="relu"), # Hidden Layer   
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(64, activation="relu"), # Hidden Layer       
      tf.keras.layers.Dropout(0.5),
      tf.keras.layers.Dense(nclasses, activation=lastAct)
  ])

  # compile
  print(nclasses)
  if nclasses==1:
    classifier.compile(
        optimizer = tf.keras.optimizers.Adam(learning_rate=lr),
        loss = tf.keras.losses.BinaryCrossentropy(from_logits = False),
        metrics = ['accuracy'])
  else:
    classifier.compile(
        optimizer = tf.keras.optimizers.Adam(learning_rate=lr),
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),
        metrics = ['accuracy'])
  # train
  history_pretrained = classifier.fit(train_ds, 
                    epochs = epochs, 
                    batch_size=batch_size, 
                    verbose=0,
                    validation_data=valid_ds,
                    callbacks=callbacks)

  loss_train, accu_train, loss_valid, accu_valid = get_loss_accu(history_pretrained)
  total_epoch = len(loss_train)
  plot_loss_accu(modelname, total_epoch, loss_train, accu_train, loss_valid, accu_valid)

  #evaluate
  test_loss, test_accu = classifier.evaluate(test_ds)
  print("Pretrain model {}: test dataset: loss={}, accuracy={}".format
          (model, np.round(test_loss,2), np.round(test_accu,2))) 

  #predict
  pred_train = classifier.predict(train_ds)
  pred_valid = classifier.predict(valid_ds)
  pred_test = classifier.predict(test_ds)

  if nclasses==1 :
    pred_train = np.int8(np.array([p>0.5 for p in pred_train]))
    pred_valid = np.int8(np.array([p>0.5 for p in pred_valid]))
    pred_test = np.int8(np.array([p>0.5 for p in pred_test]))
  else :
    pred_train = np.array([np.argmax(p) for p in pred_train])
    pred_valid = np.array([np.argmax(p) for p in pred_valid])
    pred_test  = np.array([np.argmax(p) for p in pred_test])

  return pred_train, pred_valid, pred_test

